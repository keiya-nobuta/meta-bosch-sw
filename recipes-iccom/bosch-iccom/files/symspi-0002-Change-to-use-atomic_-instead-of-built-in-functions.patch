From b593917c951023ce452993482848c3b96c041656 Mon Sep 17 00:00:00 2001
From: Keiya Nobuta <nobuta.keiya@fujitsu.com>
Date: Sun, 30 May 2021 14:55:22 +0900
Subject: [PATCH 2/3] Change to use atomic_* instead of built-in functions

Signed-off-by: Keiya Nobuta <nobuta.keiya@fujitsu.com>
---
 linux-symspi/src/symspi.c | 116 +++++++++++++++++++++++----------------------------
 1 file changed, 52 insertions(+), 64 deletions(-)

diff --git a/linux-symspi/src/symspi.c b/src/symspi.c
index 041cf33..26366e4 100644
--- a/linux-symspi/src/symspi.c
+++ b/linux-symspi/src/symspi.c
@@ -379,9 +379,9 @@
 	SYMSPI_CHECK_PRIVATE(msg, error_action)
 
 #define SYMSPI_CHECK_STATE(expected_state, error_action)		\
-	if (symspi->p->state != expected_state) {			\
+	if (atomic_read(&symspi->p->state) != expected_state) {		\
 		symspi_err("called not in %d state but in %d state."	\
-			   , expected_state, symspi->p->state);		\
+			   , expected_state, atomic_read(&symspi->p->state));	\
 		error_action;						\
 	}
 
@@ -501,8 +501,7 @@ static int symspi_update_xfer_sequence(struct symspi_dev __kernel *symspi
 		, bool force_size_change);
 inline static bool symspi_is_their_request(
 		struct symspi_dev __kernel *symspi);
-inline static char *symspi_get_state_ptr(void *symspi_dev);
-inline static char symspi_get_state(void *symspi_dev);
+inline static atomic_t *symspi_get_state_ptr(void *symspi_dev);
 inline static bool symspi_switch_strict(void *symspi_dev_ptr
 		, char expected_state
 		, char dst_state);
@@ -739,9 +738,9 @@ struct symspi_dev_private {
 	struct work_struct postprocessing_work;
 	struct work_struct recover_work;
 
-	char state;
+	atomic_t state;
 
-	int their_flag_drop_counter;
+	atomic_t their_flag_drop_counter;
 
 	bool spi_master_mode;
 	bool hardware_spi_rdy;
@@ -749,7 +748,7 @@ struct symspi_dev_private {
 	int their_flag_irq_number;
 
 	bool delayed_xfer_request;
-	bool close_request;
+	atomic_t close_request;
 	struct completion final_leave_xfer_completion;
 
 	int last_error;
@@ -976,7 +975,7 @@ int symspi_init(void __kernel *device
 
 	// initializing the fields of symspi private
 	symspi->p->magic = SYMSPI_PRIVATE_MAGIC;
-	symspi->p->close_request = true;
+	atomic_set(&symspi->p->close_request, true);
 	symspi->p->symspi = symspi;
 	symspi->p->last_error = SYMSPI_SUCCESS;
 
@@ -1035,13 +1034,13 @@ int symspi_init(void __kernel *device
 	__SYMSPI_INIT_LEVEL(WORKQUEUE_INIT);
 
 	// still cold for now
-	symspi->p->state = SYMSPI_STATE_COLD;
+	atomic_set(&symspi->p->state, SYMSPI_STATE_COLD);
 
 	// NOTE: to be selfconsistend with regular flow, we need to set up
 	//      counter to 1 here, to assume, that other side finished with
 	//      previous xfer, as long either there was no xfer before at all
 	//      either it was reset after error, so previous xfer is still done.
-	symspi->p->their_flag_drop_counter = 1;
+	atomic_set(&symspi->p->their_flag_drop_counter, 1);
 
 	// our steady configuration
 	symspi->p->spi_master_mode = SYMSPI_SPI_MASTER;
@@ -1070,8 +1069,8 @@ int symspi_init(void __kernel *device
 	// we go to normal workflow.
 	// TODO: verify close_request sequence
 	__SYMSPI_INIT_LEVEL(FULL);
-	symspi->p->close_request = false;
-	symspi->p->state = SYMSPI_STATE_IDLE;
+	atomic_set(&symspi->p->close_request, false);
+	atomic_set(&symspi->p->state, SYMSPI_STATE_IDLE);
 
 	symspi_info(SYMSPI_LOG_INFO_KEY_LEVEL, "initialization done");
 #ifdef SYMSPI_DEBUG
@@ -1113,14 +1112,13 @@ int symspi_close(void __kernel *device)
 	// NOTE: this will prevent all strict state switches (EXCEPT
 	//      leaving the XFER state), as well as API entries from
 	//      consumer code (EXCEPT for init and reset calls)
-	bool res = __atomic_compare_exchange_n(&symspi->p->close_request
-			, &expected_state, dst_state, false
-			, __ATOMIC_SEQ_CST, __ATOMIC_SEQ_CST);
-	if (!res) {
+	int old_state = atomic_cmpxchg(&symspi->p->close_request,
+				 expected_state, dst_state);
+	if (old_state == expected_state) {
 		symspi_err("device is closing already");
 		return -EALREADY;
 	}
-	if (symspi->p->state == SYMSPI_STATE_COLD) {
+	if (atomic_read(&symspi->p->state) == SYMSPI_STATE_COLD) {
 		symspi_err("device is already closed");
 		return SYMSPI_SUCCESS;
 	}
@@ -1160,7 +1158,7 @@ full:
 	// * non closeable (need to wait for hardware)
 	//   * SYMSPI_STATE_XFER
 
-	if (symspi->p->state == SYMSPI_STATE_XFER) {
+	if (atomic_read(&symspi->p->state) == SYMSPI_STATE_XFER) {
 		const unsigned long wait_jiffies = msecs_to_jiffies(
 				  SYMSPI_CLOSE_HW_WAIT_TIMEOUT_MSEC);
 		unsigned long res = wait_for_completion_timeout(
@@ -1252,7 +1250,7 @@ bool symspi_is_running(void __kernel *device)
 		return false;
 	}
 	// TODO: to update as mutex on init is done is used
-	return symspi->p->state != SYMSPI_STATE_COLD;
+	return atomic_read(&symspi->p->state) != SYMSPI_STATE_COLD;
 }
 
 // API:
@@ -1764,7 +1762,7 @@ static inline void __symspi_cancel_work_sync(
 //      false: else
 static inline bool __symspi_is_closing(struct symspi_dev *symspi)
 {
-	return symspi->p->close_request;
+	return atomic_read(&symspi->p->close_request);
 }
 
 // Starts from IDLE state. Updates our default TX data.
@@ -1895,10 +1893,10 @@ static void symspi_printout(struct symspi_dev *symspi)
 		    , "xfer accepted callback: %px"
 		    , symspi->xfer_accepted_callback);
 	symspi_info(SYMSPI_LOG_INFO_DBG_LEVEL
-		    , "symspi state: %d", (int)symspi->p->state);
+		    , "symspi state: %d", atomic_read(&symspi->p->state));
 	symspi_info(SYMSPI_LOG_INFO_DBG_LEVEL
 		    , "their flag drop counter: %d"
-		    , symspi->p->their_flag_drop_counter);
+		    , atomic_read(&symspi->p->their_flag_drop_counter));
 	if (symspi->p->spi_master_mode) {
 		symspi_info(SYMSPI_LOG_INFO_DBG_LEVEL, "master mode");
 	} else {
@@ -2066,7 +2064,7 @@ static int symspi_xfer_prepare_to_waiting_prev_sequence(
 
 	// note, spi slave will bypass waiting prev state
 	// immediately to xfer state
-	if (symspi->p->their_flag_drop_counter == 1
+	if (atomic_read(&symspi->p->their_flag_drop_counter) == 1
 			|| !symspi->p->spi_master_mode) {
 		return symspi_try_leave_waiting_prev_sequence(symspi);
 	}
@@ -2229,9 +2227,10 @@ inline static int symspi_replace_xfer(
 		symspi_err("%s: zero ptr to new xfer.", __func__);
 		return -SYMSPI_ERROR_LOGICAL;
 	}
-	if (symspi->p->state != SYMSPI_STATE_XFER_PREPARE
-			&& symspi->p->state != SYMSPI_STATE_XFER
-			&& symspi->p->state != SYMSPI_STATE_ERROR) {
+	int state = atomic_read(&symspi->p->state);
+	if (state) != SYMSPI_STATE_XFER_PREPARE
+		&& state != SYMSPI_STATE_XFER
+		&& state != SYMSPI_STATE_ERROR) {
 		symspi_err("%s: was executed while not in XFER_PREPARE"
 			   " or XFER or ERROR state.", __func__);
 		return -SYMSPI_ERROR_LOGICAL;
@@ -2260,7 +2259,7 @@ inline static int symspi_replace_xfer(
 		// previous xfer was closed, data race conditions and
 		// sync between sides loss may appear (if other side is
 		// not aware about this change).
-		if (symspi->p->state != SYMSPI_STATE_XFER
+		if (atomic_read(&symspi->p->state) != SYMSPI_STATE_XFER
 				&& !force_size_change) {
 			symspi_err("%s: sudden change in xfer size"
 				   " while not in XFER state. Will"
@@ -2341,26 +2340,18 @@ inline static bool symspi_is_their_request(
 		struct symspi_dev __kernel *symspi)
 {
 	// see their_flag_drop_counter description
-	return symspi->p->their_flag_drop_counter == 1
+	return atomic_read(&symspi->p->their_flag_drop_counter) == 1
 		    && symspi_their_flag_is_set(symspi);
 }
 
 // Small helper to get our state pointer from our general data
 //
 // NOTE: only for internal use
-inline static char *symspi_get_state_ptr(void *symspi_dev)
+inline static atomic_t *symspi_get_state_ptr(void *symspi_dev)
 {
 	return &(((struct symspi_dev *)symspi_dev)->p->state);
 }
 
-// Small helper to get our state
-//
-// NOTE: only for internal use
-inline static char symspi_get_state(void *symspi_dev)
-{
-	return ((struct symspi_dev *)symspi_dev)->p->state;
-}
-
 // Atomically swiches the state from expected_state to
 // dst_state if and only if current state == expected_state.
 //
@@ -2374,7 +2365,7 @@ inline static char symspi_get_state(void *symspi_dev)
 inline static bool symspi_switch_strict(void *symspi_dev_ptr,
 	char expected_state, char dst_state)
 {
-	char *state_ptr = symspi_get_state_ptr(symspi_dev_ptr);
+	atomic_t *state_ptr = symspi_get_state_ptr(symspi_dev_ptr);
 
 	// as closing request comes we can't do anything except
 	// leaving the XFER state
@@ -2386,9 +2377,7 @@ inline static bool symspi_switch_strict(void *symspi_dev_ptr,
 			return false;
 		}
 
-		__atomic_compare_exchange_n(state_ptr, &expected_state
-				, dst_state, false, __ATOMIC_SEQ_CST
-				, __ATOMIC_SEQ_CST);
+		atomic_cmpxchg(state_ptr, expected_state, dst_state);
 
 		// at this point we are in correct state for closing
 		// anyway ,
@@ -2397,20 +2386,22 @@ inline static bool symspi_switch_strict(void *symspi_dev_ptr,
 		return false;
 	}
 
-	bool res = __atomic_compare_exchange_n(state_ptr, &expected_state
-			, dst_state, false, __ATOMIC_SEQ_CST
-			, __ATOMIC_SEQ_CST);
-	if (res) {
-		symspi_trace_raw(
-				"Switched from %d to %d", (int)expected_state
+	int old_state = atomic_cmpxchg(state_ptr, expected_state, dst_state);
+	bool res;
+
+	if (old_state != expected_state) {
+		symspi_info_raw(SYMSPI_LOG_INFO_DBG_LEVEL
+				, "Tried switch from %d to %d, but failed"
+				, (int)old_state
 				, (int)dst_state);
+		symspi_info_raw(SYMSPI_LOG_INFO_DBG_LEVEL
+				, "Current state: %d", atomic_read(state_ptr));
+		res = false;
 	} else {
-		symspi_trace_raw(
-				"Tried switch from %d to %d, but failed"
-				, (int)expected_state
+		symspi_info_raw(SYMSPI_LOG_INFO_DBG_LEVEL
+				, "Switched from %d to %d", old_state
 				, (int)dst_state);
-		symspi_trace_raw(
-				"Current state: %d", (int)(*state_ptr));
+		res = true;
 	}
 	return res;
 }
@@ -2426,8 +2417,8 @@ inline static char symspi_switch_state_val_forced(void *symspi_dev_ptr
 {
 	symspi_info(SYMSPI_LOG_INFO_DBG_LEVEL
 		    , "Forced switching to %d.", (int)dst_state);
-	char *state_ptr = symspi_get_state_ptr(symspi_dev_ptr);
-	return __atomic_exchange_n(state_ptr, dst_state, __ATOMIC_SEQ_CST);
+	atomic_t *state_ptr = symspi_get_state_ptr(symspi_dev_ptr);
+	return atomic_xchg(state_ptr, dst_state);
 }
 
 // TODO: not used for now, need performance tests to decide
@@ -2485,8 +2476,7 @@ static int symspi_do_xfer(struct symspi_dev *symspi)
 #endif
 
 	// dropping their flag falling edge counter right before xfer
-	__atomic_store_n(&symspi->p->their_flag_drop_counter
-			 , 0, __ATOMIC_SEQ_CST);
+	atomic_set(&symspi->p->their_flag_drop_counter, 0);
 
 	// note, SPI_READY flow is enabled/disabled at SPI init time
 	int res = spi_async(symspi->spi, &symspi->p->spi_msg);
@@ -2615,8 +2605,7 @@ static int symspi_recovery_sequence(struct symspi_dev *symspi)
 	}
 
 	// dropping their error indication
-	__atomic_store_n(&symspi->p->their_flag_drop_counter
-			 , 1, __ATOMIC_SEQ_CST);
+	atomic_set(&symspi->p->their_flag_drop_counter, 1);
 
 	symspi->p->last_error = SYMSPI_SUCCESS;
 	if (report) {
@@ -2937,7 +2926,7 @@ error_handling:
 static int symspi_try_to_error_sequence(struct symspi_dev *symspi
 					, int internal_error)
 {
-	bool other_side_error = (symspi->p->their_flag_drop_counter > 1);
+	bool other_side_error = (atomic_read(&symspi->p->their_flag_drop_counter) > 1);
 
 	if (internal_error != SYMSPI_SUCCESS || other_side_error) {
 		int err_no = (internal_error == SYMSPI_SUCCESS)
@@ -3296,7 +3285,7 @@ static irqreturn_t symspi_their_flag_isr(int irq, void *symspi_device)
 #endif
 	symspi_trace("Their flag ISR.");
 
-	if (symspi->p->state == SYMSPI_STATE_COLD) {
+	if (atomic_read(&symspi->p->state) == SYMSPI_STATE_COLD) {
 		return IRQ_HANDLED;
 	}
 
@@ -3326,7 +3315,7 @@ static void symspi_their_flag_drop_isr_sequence(
 #ifdef SYMSPI_DEBUG
 	SYMSPI_CHECK_DEVICE_AND_PRIVATE("No device provided.", return);
 #endif
-	int *counter_ptr = &(symspi->p->their_flag_drop_counter);
+	atomic_t *counter_ptr = &(symspi->p->their_flag_drop_counter);
 
 	// NOTE: we could use the kernel provided function, but
 	//          with current kernel version, all atomic operations
@@ -3338,8 +3327,7 @@ static void symspi_their_flag_drop_isr_sequence(
 	//
 	// For now will use GCC atomic builtin.
 
-	const int counter = __atomic_add_fetch(counter_ptr, 1
-					       , __ATOMIC_SEQ_CST);
+	const int counter = atomic_inc_return(counter_ptr);
 
 	// ISR does nothing but counter management on SPI slave side
 	if (counter == 1 && symspi->p->spi_master_mode) {
@@ -3474,7 +3462,7 @@ static void symspi_destroy_device(struct symspi_dev *symspi)
 		return;
 	}
 	if (!IS_ERR_OR_NULL(symspi->p)
-			&& symspi->p->state != SYMSPI_STATE_COLD) {
+			&& atomic_read(&symspi->p->state) != SYMSPI_STATE_COLD) {
 		symspi_close((void*)symspi);
 	}
 
-- 
2.25.1

